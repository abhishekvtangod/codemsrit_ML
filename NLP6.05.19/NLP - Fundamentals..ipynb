{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing - NLP\n",
    "\n",
    "NLP is one of the fields that, together with ML, make up the bigger part of AI. It deals with understanding between computers and humans, and interactions between them as such.\n",
    "\n",
    "NLP is used in fields like Information Engineering, Web Mining, Structure Analysis and Generation, Text Analysis, Speech Recognition, etc.\n",
    "\n",
    "### Examples of good NLP engines - Siri, Google Assistant, Alexa, etc.\n",
    "\n",
    "The basic idea behind NLP is to bridge the gap of understanding between humans and machines - making machines independant and to automate everything with textual or voice data, instead of manual interference / effort.\n",
    "\n",
    "### NLP, in a nutshell, is the key to understanding and building a functional AI. \n",
    "\n",
    "Any AI depends on the interactions made by a human and a machine understanding them as such. Natural Language refers to the making, structuring and understanding of a language that we, as humans have developed. We are able to understand each other seamlessly, so long the topic of discussion is generalistic. Imagine teaching the concept of language and understanding to a Machine. NLP basically caters to this genre of intelligence in Data Sciences.\n",
    "\n",
    "An Advanced version of NLP (NLU) requires indepth understanding of knowledge and logic to form and understand sentences, ability to parse speech data, and recognize each person using 'memories'. Technically, being able to train a machine to understand and learn the way humans do - only much, much faster. So with this in mind, let us take our first steps towards the true building blocks of Artificial Intelligence - the Natural Language Processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As such, let us start with understanding our data -\n",
    "\n",
    "As much as we shied away from categorical data in Machine Learning, NLP deals with \"natural language\" data, i.e., mostly categorical data. As such, we need to improvise and develop upon methods that would help us get a better idea of what we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim.models\n",
    "import networkx as nx\n",
    "import re\n",
    "import pandas as pd\n",
    "import regex as regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this introduction to NLP, we will be dealing with specific aspects of data : \n",
    "#### Processing Documents,   Building a Sample Information Retrieval model,   Sentiment Analysis,   and Text Analysis. \n",
    "### Finally, we conclude by tinkering on Speech data.\n",
    "\n",
    "\n",
    "# NLP - Part 1 : Processing Documents\n",
    "\n",
    "#### Let us take a look at our dataset, and basic types of pre-processing we would need to do on these.\n",
    "- Removing frequent / unimportant words (stopwords)\n",
    "- Standardizing all words that have the same root (stemming / lemmatizing)\n",
    "- Getting Sentences/phrases in the document\n",
    "- Ranking terms / words in the document in order for easier lookup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from sklearn import metrics\n",
    "from nltk import ngrams\n",
    "from stemming.porter2 import stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_training_data, raw_testing_data = [],[]\n",
    "\n",
    "with open(\"train.txt\") as f:\n",
    "    train=f.readlines()\n",
    "    ttrain=f.read()\n",
    "for item in train:\n",
    "    raw_training_data.append(item.strip())\n",
    "    \n",
    "with open(\"test.txt\") as f:\n",
    "    test=f.readlines()\n",
    "for item in test:\n",
    "    raw_testing_data.append(item.strip())\n",
    "\n",
    "#get an idea of the data\n",
    "freq=pd.Series(' '.join(train[1]).split()).value_counts()[:10]\n",
    "freq=list(freq.index)\n",
    "wordcount = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate duplicates, split by punctuation and use case demiliters\n",
    "for word in ttrain.lower().split('\\t', 1):\n",
    "    word = word.replace(\".\",\"\")\n",
    "    word = word.replace(\",\",\"\")\n",
    "    word = word.replace(\":\",\"\")\n",
    "    word = word.replace(\"\\\"\",\"\")\n",
    "    word = word.replace(\"!\",\"\")\n",
    "    word = word.replace(\"*\",\"\")\n",
    "    if word not in wordcount:\n",
    "        wordcount[word] = 1\n",
    "    else:\n",
    "        wordcount[word] += 1\n",
    "#most common words / features\n",
    "word_counter = collections.Counter(wordcount)\n",
    "\n",
    "#clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the files taken, we see that entirety of the dataset is categorical data. Hence, some cleaning is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(raw_data):\n",
    "    \n",
    "    #split into labels and text\n",
    "    labels=[lab.split('\\t', 1)[0] for lab in raw_data]\n",
    "    training_data= [item.split('\\t', 1)[1] for item in raw_data]\n",
    "    \n",
    "    labels,training_data\n",
    "    \n",
    "    #convert to lowercase, stem / lemmatize\n",
    "    training_data = [i.lower() for i in training_data]\n",
    "    training_data = [\" \".join([stem(word) for word in sentence.split(\" \")]) for sentence in training_data]\n",
    "        \n",
    "    #replace links, email_id, currencies, entities etc\n",
    "    training_data=[re.sub(r'[\\w\\.-]+@[\\w\\.-]+', \"$EMAIL_ID\", i) for i in training_data]\n",
    "    training_data=[re.sub(r\"(<?)http:\\S+\", \"$URL\", i) for i in training_data]\n",
    "    training_data=[re.sub(r\"\\$\\d+\", \"$CURR\", i) for i in training_data]\n",
    "    training_data=[re.sub(r'\\b\\d+\\b', \"$NUM\", i) for i in training_data]\n",
    "    training_data=[re.sub(r'\\b(me|her|him|us|them|you)\\b', \"$ENTITIES\", i) for i in training_data]\n",
    "    \n",
    "    \n",
    "    #remove punctuation, special chars, tokenize data\n",
    "    training_data = [regex.sub(r\"[^\\P{P}$]+\", \" \", i) for i in training_data]\n",
    "    training_data = [re.sub(r\"[^0-9A-Za-z/$' ]\", \" \", i) for i in training_data]\n",
    "    \n",
    "    #regularize data w.r.t days, times, months and year\n",
    "    regex_match_days= r'monday|tuesday|wednesday|thursday|friday|saturday|sunday'\n",
    "    regex_match_times= r'morning|afternoon|evening'\n",
    "    regex_match_events= r'after|before|during'\n",
    "    regex_match_month= r'january|february|march|april|may|june|july|august|september|october|november|december'\n",
    "    \n",
    "    training_data = [re.sub(regex_match_days, \"$DAY\", i) for i in training_data]\n",
    "    training_data = [re.sub(regex_match_times, \"$TIMES\", i) for i in training_data]\n",
    "    training_data = [re.sub(regex_match_events, \"$EVENTS\", i) for i in training_data]\n",
    "    training_data = [re.sub(regex_match_month, \"$MONTH\", i) for i in training_data]\n",
    "    \n",
    "    #remove extra spaces and blanks\n",
    "    training_data = [item.strip() for item in training_data]\n",
    "    \n",
    "    #return cleaned data\n",
    "    return training_data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We see that data has been read into, and cleaning has been completed.\n",
    "\n",
    "### Now we need to see how to handle cleaned data - we will develop a small phrase extraction algorithm to get phrases from the data. We will train our data in terms of these phrases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrases(text, n):\n",
    "    \n",
    "    #can define in two ways - one way is to get both bigrams and tri-grams from a single pass of the function\n",
    "    #depends on the number of values unpacked by the host OS, so it has been rewritten for a dual pass and as a multi n-gram formation function\n",
    "    \n",
    "    n_grams=ngrams(text.split(),n)\n",
    "    gram_list = []\n",
    "    for grams in n_grams:\n",
    "        gram_list.append('_'.join(map(str,grams)))\n",
    "    gram_string = ' '.join(gram_list)\n",
    "    #print gram_string\n",
    "    return gram_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To use these phrases as our training data, we need to rank them and make more 'numerical' weights out of them, so that a Machine Learning classifier like SVM or Naive Bayes Classifier can be used directly on our data.\n",
    "\n",
    "TF-IDF (Term Frequency - Inverse Document Frequency) is one such ranking metric. Basically, the words that appear really often in all documents are given no weights. The rarer the words are, the higher they will have an IDF value. However, the number of times a word is specified in that particular (sentence / phrase / paragraph) document decides it's Term Frequency.\n",
    "\n",
    "Multiplication of TF with IDF gives a numeric value, which can be used to rank documents. Higher TF-IDF value means higher ranking of the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'sklearn.cross_validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3cc4ef8a47c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'sklearn.cross_validation'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(data):\n",
    "    \n",
    "    vector_model= TfidfVectorizer(min_df=1) #single dataframe\n",
    "    X = vector_model.fit_transform(data) #fit model and transform data to required vector\n",
    "    \n",
    "    #only one axis holds the ranking -> X axis\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the cleaned data : \n",
    "training_data, training_labels = clean(raw_training_data)\n",
    "testing_data, testing_labels = clean(raw_testing_data)\n",
    "\n",
    "X = tf_idf(training_data+testing_data)\n",
    "Y = training_labels+testing_labels\n",
    "\n",
    "#set random_state to 42 for reproducible results\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will train our model by using the SVM classifier. It is a simple binary classifier, fit for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since we do not know what kind of phrases we are working with, let us go with the idea of unigrams, bigrams and trigrams.\n",
    "\n",
    "This next cell deals with training and model fitting for a unigram phrase - basically, presence of a single word. Although it does not make much sense to have something like this, this acts as a dummy idea for the highest un-augmented data accuracy for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e5d81ed1c280>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rbf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"SVM classification : \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Accuracy : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\\nTesting Accuracy : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Training Accuracy : 0.885 , Testing Accuracy : 0.781\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "svm=SVC(C=2100, kernel = 'rbf')\n",
    "svm.fit(x_train, y_train)\n",
    "print (\"SVM classification : \\n\")\n",
    "print (\"Training Accuracy : \",svm.score(x_train, y_train),\"\\nTesting Accuracy : \",svm.score(x_test, y_test))\n",
    "# Training Accuracy : 0.885 , Testing Accuracy : 0.781\n",
    "#bi-gram and tri-gram SVM model\n",
    "total_data = training_data+testing_data\n",
    "total_labels = training_labels+testing_labels\n",
    "\n",
    "bi_gram_data = [get_phrases(item,2) for item in total_data]\n",
    "tri_gram_data = [get_phrases(item,3) for item in total_data]\n",
    "\n",
    "X2, Y2 = tf_idf(bi_gram_data), total_labels\n",
    "X3, Y3 = tf_idf(tri_gram_data), total_labels\n",
    "\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(X2, Y2, test_size=0.3, random_state=42)\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(X3, Y3, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us build the same model fit for bigrams and trigrams now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Max C value :  14600\n",
      " Max C value trigrams :  23200\n"
     ]
    }
   ],
   "source": [
    "best_c={}\n",
    "for increment in range(1000, 15000, 100):\n",
    "    svm=SVC(C=increment, kernel = 'rbf')\n",
    "    svm.fit(x_train1, y_train1)\n",
    "    tr_ac, te_ac = svm.score(x_train1, y_train1), svm.score(x_test1, y_test1)\n",
    "    best_c[str(increment)]=te_ac\n",
    "import operator\n",
    "print \" Max C value : \",max(best_c.iteritems(),key=operator.itemgetter(1))[0]\n",
    "\n",
    "#SVM C value for tri-grams, takes time to compute. Comment it out till the print statement if you dont want to run time constraints\n",
    "best_tri_c={}\n",
    "for increment in range(10000, 30000, 100):\n",
    "    svm=SVC(C=increment, kernel = 'rbf')\n",
    "    svm.fit(x_train1, y_train1)\n",
    "    tr_ac, te_ac = svm.score(x_train1, y_train1), svm.score(x_test1, y_test1)\n",
    "    best_tri_c[str(increment)]=te_ac\n",
    "import operator\n",
    "print \" Max C value trigrams : \",max(best_tri_c.iteritems(),key=operator.itemgetter(1))[0] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classification for Bi-Grams : \n",
      "Training accuracy :  0.9797172710510141\n",
      "Testing accuracy :  0.7827956989247312\n",
      "\n",
      "SVM Classification for Tri-Grams : \n",
      "Training accuracy :  0.9929317762753535\n",
      "Testing accuracy :  0.739068100358423\n"
     ]
    }
   ],
   "source": [
    "#bi-gram SVM max value as C = 14600.\n",
    "svm=SVC(C=14600, kernel='rbf')\n",
    "svm.fit(x_train1, y_train1)\n",
    "print \"SVM Classification for Bi-Grams : \"\n",
    "print \"Training accuracy : \",svm.score(x_train1, y_train1) # 0.9797\n",
    "print \"Testing accuracy : \",svm.score(x_test1, y_test1) # 0.7828\n",
    "\n",
    "#tri-gram SVM max value as C = 23200.\n",
    "svm=SVC(C=23200, kernel='rbf')\n",
    "svm.fit(x_train2, y_train2)\n",
    "print \"\\nSVM Classification for Tri-Grams : \"\n",
    "print \"Training accuracy : \",svm.score(x_train2, y_train2) # 0.9929\n",
    "print \"Testing accuracy : \",svm.score(x_test2, y_test2) # 0.7391"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From this, we see that even though the training accuracy is really high, our testing accuracy is reducing. We can assume from this that since the data we have is categorical, formation of higher order n_grams would reduce the model accuracy.\n",
    "\n",
    "Hence, it is a better idea for us to use the bi_gram model fit rather than the tri_gram model fit, because bi_grams give better accuracy difference between the train / test values, and still maintain an intuitive logic for presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Part 2 : Building a Sample Information Retrieval Model\n",
    "\n",
    "#### Let us take a look at our dataset, and basic types of pre-processing we would need to do on these.\n",
    "- Removing / transformation of data on the basis of encoding\n",
    "- Extract words from the encoded data\n",
    "- Removing Stopwords and forming regex compilation patterns to look for in data\n",
    "- Building the solution in a structured format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My data is encoded in a different format for the given bytestream, so I will decode the data to direct python objects. For those of you who do not have this problem, skip to the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPOINTS = [\"delivery_currency\", \"delivery_amount\", \"delivery_rounding\", \"return_currency\", \"return_amount\", \"return_rounding\"]\n",
    "\n",
    "def read_json(filename):\n",
    "\n",
    "    with open(filename,\"r\") as f_p:\n",
    "        list_isda_data = json.load(f_p)\n",
    "    print \"LIST DATA : unicode characters detected. Manually removing them.\"\n",
    "    print \"Cleaned JSON data...\"\n",
    "    cleaned_data = json_load_byteified(open(filename));\n",
    "    print \"Sending cleaned data for information extraction\"\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "def json_load_byteified(file_handle): #if you want to check with filenames\n",
    "    return convbytes(json.load(file_handle, object_hook=convbytes),ignore_dicts=True)\n",
    "\n",
    "def json_loads_byteified(json_text): #if you want to check with text data without filenames\n",
    "    return convbytes(json.loads(json_text, object_hook=convbytes),ignore_dicts=True)\n",
    "\n",
    "def convbytes(data, ignore_dicts=False):\n",
    "    if isinstance(data,unicode):\n",
    "        return data.encode('utf-8')\n",
    "    if isinstance(data, list):\n",
    "        return [ convbytes(item, ignore_dicts=True) for item in data ]\n",
    "    if isinstance(data, dict) and not ignore_dicts:\n",
    "        return { convbytes(key, ignore_dicts=True): convbytes(value, ignore_dicts=True) for key,value in data.iteritems() }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(data):\n",
    "    #we have the cleaned data from read_json()\n",
    "    #we need to get the data we need for stopwords, i.e, the text needs to be processed.\n",
    "\n",
    "    #SOLUTION and Explanation : \n",
    "    #remove stopwords first and then use regex.compile and pattern.check to find currency, rounding type and amount.\t\n",
    "    #amount appears right after currency, so extract format should be `currency`:`amount`. use regex.compile for this.\n",
    "    #if currency regex matches more than once, then compare return_curr with delivery_curr\n",
    "    #if \"rounding\" occurs anywhere, note the type. if type is not mentioned, take nearest as default.\n",
    "    #if \"rounding\" occurs multiple times, take the first and last occurrance of rounding, one will be deliver, the other will be return\n",
    "    #match amount regex if it appears multiple times. first occurrance is delivery, next occurrance is return.\n",
    "\n",
    "    #start by finding stopwords. Take the stopwords file. Note: not using the stopwords from nltk.corpus since it has a few common words missing.\n",
    "    stopwords,finwords=[],[]\n",
    "    with open(\"stopwords.txt\",\"r+\") as stop:\n",
    "        for line in stop:\n",
    "            for word in line.split():\n",
    "                stopwords.append(word)\n",
    "\n",
    "    #extract \"text\" from the data object parsed from read_json().\n",
    "    #remove stopwords from \"text\" and tokenize words, form bi-grams for regex matching at currency and rounding.\n",
    "    predicted_output=[]\n",
    "    for i in range(len(data)):\n",
    "        text_data=[datum['text']for datum in data][i].lower()\n",
    "        print \"\\nTEXT DATA : \\n\",text_data\n",
    "        word_tokens=word_tokenize(text_data)\n",
    "        filtered_sentence=[w for w in word_tokens if not w in stopwords]\n",
    "        #print \"\\nGetting bi-grams\"\n",
    "        #bi-grams are a text input mechanism, so we will do a copy from list -> text as well to send to the param.\n",
    "\n",
    "        st_text=\"\"\n",
    "        info_list=[]\n",
    "        for item in filtered_sentence:\n",
    "            st_text=st_text+\" \"+item\n",
    "        info_list = get_bigrams(st_text)\n",
    "        #bi-grams have been formed, returned. Now a simple return and regex check for the phrases we are looking for :\n",
    "\n",
    "        curr_type_and_amount, rounding_type, it, it1 = [],[],0,0\n",
    "        for item in info_list:\n",
    "            del_curr = re.match(\"[a-z]{3}_[0-9]\",item)\n",
    "            if del_curr:\n",
    "                curr_type_and_amount.insert(it+1,item)\n",
    "            if item == \"rounded_up\" or item == \"rounded_down\" or item == \"rounded_nearest\" or item== \"amount_rounded\":\n",
    "                rounding_type.insert(it1+1,item)\n",
    "        #currency type and amount has been found. Let us move to delivery type and amount.\n",
    "        #if currency type and amount is found more than once, the latter type and amount is return, not delivery\n",
    "        #if rounded type is found more than once, former is delivery, latter is return\n",
    "        #if found only once, that means return param = del param.\n",
    "\n",
    "        #print \"CURR TYPE AND AMOUNT : \\n\",curr_type_and_amount\n",
    "        #print \"ROUNDING TYPE: \\n\",rounding_type\n",
    "        #search for number of items and decide what item could be where.\n",
    "\n",
    "        # INTUITION p1:\n",
    "\n",
    "        #1st item in list1 would be currency type + amount, 2nd list would be rounding type.\n",
    "        #if there is a 2nd item in list1, it could be return curr/amount.\n",
    "        #if there is a 2nd item in list2, it could be return rounding type.\n",
    "\n",
    "        # INTUITION p2:\n",
    "\n",
    "        #if there is only one item in both lists, return param = delivery param.\n",
    "        #if there is only one item in list1, and no items in list2, return param = delivery param AND rounding type =  nearest.\n",
    "        datap=OrderedDict()\n",
    "        \"\"\"\"\"\"\n",
    "        datap[\"delivery_currency\"]=\"\"\n",
    "        datap[\"delivery_amount\"]=\"\"\n",
    "        datap[\"delivery_rounding\"]=\"\"\n",
    "        datap[\"return_currency\"]=\"\"\n",
    "        datap[\"return_amount\"]=\"\"\n",
    "        datap[\"return_rounding\"]=\"\"\n",
    "\n",
    "        if len(rounding_type) == 0 or \"amount_rounded\" in rounding_type:\n",
    "            datap[\"delivery_rounding\"]=\"nearest\"\n",
    "            datap[\"return_rounding\"]=\"nearest\"\n",
    "        else :\n",
    "            datap[\"delivery_rounding\"]=rounding_type[0][8:] \n",
    "            datap[\"return_rounding\"]=rounding_type[len(rounding_type)-1][8:]\n",
    "\n",
    "        datap[\"delivery_currency\"]=curr_type_and_amount[0][0:3].upper()\n",
    "        datap[\"delivery_amount\"]=curr_type_and_amount[0][4:]\n",
    "\n",
    "        datap[\"return_currency\"]=curr_type_and_amount[len(curr_type_and_amount)-1][0:3].upper()\n",
    "        datap[\"return_amount\"]=curr_type_and_amount[len(curr_type_and_amount)-1][4:]\n",
    "\n",
    "        for key,value in datap.items():\n",
    "            print key,\": \",value\n",
    "\n",
    "        for datum in data:\n",
    "            predicted_output.append(datap)\n",
    "    final_output = []\n",
    "    for i in range(len(predicted_output)):\n",
    "        if i%len(data)==0:\n",
    "            final_output.append(predicted_output[i])\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams(text):\n",
    "    bi_grams=ngrams(text.split(),2) #set number type of n-grams. We want bi-grams, hence we are using '2' as our param.\n",
    "    l=[]\n",
    "    for grams in bi_grams:\n",
    "        l.append('_'.join(map(str,grams)))\n",
    "#f1= ' '.join(l) = string output for the bi-grams. We want a list output to re.match() for delivery amount, etc.\n",
    "#print l\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter filename you want to use for the IR module : isda_data.json\n",
      "LIST DATA : unicode characters detected. Manually removing them.\n",
      "Cleaned JSON data...\n",
      "Sending cleaned data for information extraction\n",
      "\n",
      "TEXT DATA : \n",
      "rounding. the delivery amount and the return amount will be rounded to the nearest integral multiple o f eur 100,000; provided that if an amount corresponds to the exact half o f such multiple, then it will be rounded up; and provided further that, for the purpose o f the calculation o f the return amount where a party's credit support amount is, or is deemed to be, zero, the return amount shall not be rounded.\n",
      "delivery_currency :  EUR\n",
      "delivery_amount :  100,000\n",
      "delivery_rounding :  nearest\n",
      "return_currency :  EUR\n",
      "return_amount :  100,000\n",
      "return_rounding :  nearest\n",
      "\n",
      "TEXT DATA : \n",
      "rounding. the delivery amount and the return amount will be rounded to the nearest integral multiple of usd 10,000 provided that if an amount corresponds to the exact half of such multiple, then it will be rounded up; and provided further that, for the purpose of the calculation of the return amount where a party's credit support amount is, or is deemed to be, zero, the return amount shall not be rounded.\n",
      "delivery_currency :  USD\n",
      "delivery_amount :  10,000\n",
      "delivery_rounding :  nearest\n",
      "return_currency :  USD\n",
      "return_amount :  10,000\n",
      "return_rounding :  nearest\n",
      "\n",
      "TEXT DATA : \n",
      "\"rounding\". the delivery amount will be rounded up to the nearest integral multiple of gbp 10,000 and the return amount will be rounded down to the nearest integral multiple of gbp 10,000.\n",
      "delivery_currency :  GBP\n",
      "delivery_amount :  10,000\n",
      "delivery_rounding :  up\n",
      "return_currency :  GBP\n",
      "return_amount :  10,000\n",
      "return_rounding :  down\n",
      "\n",
      "ACCURACY : \n",
      "\n",
      "('delivery_currency', 1.0)\n",
      "('delivery_amount', 1.0)\n",
      "('delivery_rounding', 1.0)\n",
      "('return_currency', 1.0)\n",
      "('return_amount', 1.0)\n",
      "('return_rounding', 1.0)\n"
     ]
    }
   ],
   "source": [
    "def evaluate(input_data, predicted_output):\n",
    "    result = defaultdict(lambda: 0)\n",
    "    for i, input_instance in enumerate(input_data):\n",
    "        for key in DATAPOINTS:\n",
    "            #print \"INPUT INSTANCE :\",input_instance[key],\"\\nPred out: \",predicted_output[i/3][key],\"\\n\\n\"\n",
    "            if input_instance[key] == predicted_output[i][key]:\n",
    "                result[key] += 1\n",
    "\n",
    "    # compute the accuracy for each datapoint\n",
    "    print \"\\nACCURACY : \\n\"\n",
    "    for key in DATAPOINTS:\n",
    "        print(key, 1.0 * result[key] / len(input_data))\n",
    "\n",
    "    return result\n",
    "fname=raw_input(\"Enter filename you want to use for the IR module : \")\n",
    "data=read_json(fname)\n",
    "pred_out=extract(data)\n",
    "result=evaluate(data,pred_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Part 3 : Sentiment Analysis\n",
    "\n",
    "#### Let us take at what we would need to do for this.\n",
    "- 3 Basic types of sentiments\n",
    "- Compound data\n",
    "- Use of  VADER for Sentiment Analysis\n",
    "- Unsupervised dataset classification with -1,0,+1 classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis and the Stigma behind it :\n",
    "\n",
    "SA has always been a buzz word so long the community at large has been concerned. It is perhaps the most well known version of NLP. It works on the basis of Opinion Mining, understanding the basic attitude of the speaker.\n",
    "\n",
    "We will make our job easier using the VADER library, although SA has multiple hard learning aspects to it. VADER works on rule-based lexicon for SA, working on their semantic orientation, giving it a rating of positive / negative / neutral and a compound score. \n",
    "\n",
    "Compound Score is nothing but sum of all the lexicon ratings in that particular document / sentence, normalized between -1 to 1 (Extreme Negative to Extreme Positive). \n",
    "\n",
    "- On average, a positive sentiment has a Compound Score > 0.05,\n",
    "- A negative sentiment has a Compound score <= -0.05.\n",
    "- A neutral sentiment lies between these two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-12-81d21853c065>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-81d21853c065>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    print \"Sentence Sent : \",sentence\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
    "from io import open\n",
    "\n",
    "def sentiment_scores(sentence): \n",
    "  \n",
    "    sid_obj = SentimentIntensityAnalyzer() \n",
    "  \n",
    "    # polarity_scores method of SentimentIntensityAnalyzer - has positive, negative and compound scores\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence) \n",
    "    \n",
    "    print \"Sentence Sent : \",sentence\n",
    "    print \"Overall sentiment dictionary is : \", sentiment_dict\n",
    "    print \"sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\" \n",
    "    print \"sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\" \n",
    "    print \"sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\" \n",
    "  \n",
    "    print \"Sentence Overall Rated As\", \n",
    "  \n",
    "    if sentiment_dict['compound'] >= 0.05 : \n",
    "        print(\"Positive\") \n",
    "  \n",
    "    elif sentiment_dict['compound'] <= - 0.05 : \n",
    "        print(\"Negative\") \n",
    "  \n",
    "    else : \n",
    "        print(\"Neutral\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us check the SA code with random sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Part 4 : Text Analysis\n",
    "\n",
    "#### Let us take at what we would need to do for this.\n",
    "- Remove delimiters\n",
    "- Clean the text , preprocess it\n",
    "- Stem (or Lemmatize)\n",
    "- Convert it all to lower case for standardization\n",
    "- Tokenize, make a bag of words\n",
    "- Split datasets into training / testing\n",
    "- Build a Prediction Model (RF Classifier), Test your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/viole/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np   \n",
    "import pandas as pd  \n",
    "  \n",
    "# Import dataset \n",
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t')  \n",
    "import re  \n",
    "  \n",
    "# Natural Language Tool Kit \n",
    "import nltk  \n",
    "  \n",
    "nltk.download('stopwords') \n",
    "  \n",
    "# to remove stopword \n",
    "from nltk.corpus import stopwords \n",
    "  \n",
    "# for Stemming propose  \n",
    "from nltk.stem.porter import PorterStemmer \n",
    "  \n",
    "# Initialize empty array \n",
    "# to append clean text  \n",
    "corpus = []  \n",
    "  \n",
    "# 1000 (reviews) rows to clean \n",
    "for i in range(0, 1000):  \n",
    "      \n",
    "    # column : \"Review\", row ith \n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])  \n",
    "      \n",
    "    # convert all cases to lower cases \n",
    "    review = review.lower()  \n",
    "      \n",
    "    # split to array(default delimiter is \" \") \n",
    "    review = review.split()  \n",
    "      \n",
    "    # creating PorterStemmer object to \n",
    "    # take main stem of each word \n",
    "    ps = PorterStemmer()  \n",
    "      \n",
    "    # loop for stemming each word \n",
    "    # in string array at ith row     \n",
    "    review = [ps.stem(word) for word in review \n",
    "                if not word in set(stopwords.words('english'))]  \n",
    "                  \n",
    "    # rejoin all string array elements \n",
    "    # to create back into a string \n",
    "    review = ' '.join(review)   \n",
    "      \n",
    "    # append each string to create \n",
    "    # array of clean text  \n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# \"max_features\" is attribute to experiment with to get better results \n",
    "cv = CountVectorizer(max_features = 1500)  \n",
    "\n",
    "X = cv.fit_transform(corpus).toarray()  \n",
    "y = dataset.iloc[:, 1].values \n",
    "\n",
    "from sklearn.cross_validation import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=501, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "  \n",
    "# n_estimators can be said as number of trees\n",
    "model = RandomForestClassifier(n_estimators = 501, criterion = 'entropy')                              \n",
    "model.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Predictions !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test) \n",
    "  \n",
    "y_pred "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an Idea of how many of the sentences were classified correctly ! - Using Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[92 29]\n",
      " [36 93]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "  \n",
    "# TP, FN, FP, TN\n",
    "cm = confusion_matrix(y_test, y_pred) \n",
    "print cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "  \n",
    "def count_words(text):                   #counts word frequency \n",
    "    skips = [\".\", \", \", \":\", \";\", \"'\", '\"'] \n",
    "    for ch in skips: \n",
    "        text = text.replace(ch, \"\") \n",
    "    word_counts = {} \n",
    "    for word in text.split(\" \"): \n",
    "        if word in word_counts: \n",
    "            word_counts[word]+= 1 \n",
    "        else: \n",
    "            word_counts[word]= 1 \n",
    "    return word_counts \n",
    "  \n",
    "    # >>>count_words(text)  You can check the function \n",
    "  \n",
    "  \n",
    "def count_words_fast(text):      #counts word frequency using Counter from collections \n",
    "    text = text.lower() \n",
    "    skips = [\".\", \", \", \":\", \";\", \"'\", '\"'] \n",
    "    for ch in skips: \n",
    "        text = text.replace(ch, \"\") \n",
    "    word_counts = Counter(text.split(\" \")) \n",
    "    return word_counts\n",
    "\n",
    "def read_book(title_path):  #read a book and return it as a string \n",
    "    with open(title_path, \"r\", encoding =\"utf8\") as current_file: \n",
    "        text = current_file.read() \n",
    "        text = text.replace(\"\\n\", \"\").replace(\"\\r\", \"\") \n",
    "        return text\n",
    "\n",
    "def word_stats(word_counts):     # word_counts = count_words_fast(text)    \n",
    "    num_unique = len(word_counts) \n",
    "    counts = word_counts.values() \n",
    "    return (num_unique, counts) \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
